{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5c86a85",
   "metadata": {},
   "source": [
    "# SimSiam Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9520bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from torchvision import models\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "DATA_ROOT = \"/kaggle/input/ssl-dataset/ssl_dataset\"\n",
    "CHECKPOINT_PATH = \"/kaggle/input/simsaimmodel/pytorch/default/1/simsiam_r18_epoch50.pth\"  \n",
    "VAL_FOLDER = \"val.X\"\n",
    "LABELS_PATH = os.path.join(DATA_ROOT, \"Labels.json\")\n",
    "BATCH_SIZE = 64\n",
    "IMG_SIZE = 96  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd42fda",
   "metadata": {},
   "source": [
    "## Class Formation and Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f2bc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimSiamBackbone(nn.Module):\n",
    "    def __init__(self, checkpoint_path):\n",
    "        super().__init__()\n",
    "        class ProjectionMLP(nn.Module):\n",
    "            def __init__(self, in_dim=512, hidden_dim=512, out_dim=512):\n",
    "                super().__init__()\n",
    "                self.layer1 = nn.Sequential(\n",
    "                    nn.Linear(in_dim, hidden_dim, bias=False),\n",
    "                    nn.BatchNorm1d(hidden_dim),\n",
    "                    nn.ReLU(inplace=True)\n",
    "                )\n",
    "                self.layer2 = nn.Sequential(\n",
    "                    nn.Linear(hidden_dim, out_dim, bias=False),\n",
    "                    nn.BatchNorm1d(out_dim)\n",
    "                )\n",
    "\n",
    "            def forward(self, x):\n",
    "                x = self.layer1(x)\n",
    "                x = self.layer2(x)\n",
    "                return x\n",
    "\n",
    "        class SimSiam(nn.Module):\n",
    "            def __init__(self):\n",
    "                super().__init__()\n",
    "                base = models.resnet18(pretrained=False)\n",
    "                self.encoder = nn.Sequential(*list(base.children())[:-1])  \n",
    "                feat_dim = base.fc.in_features  \n",
    "                self.projector = ProjectionMLP(in_dim=feat_dim,\n",
    "                                               hidden_dim=feat_dim,\n",
    "                                               out_dim=feat_dim)\n",
    "              \n",
    "\n",
    "            def forward_backbone(self, x):\n",
    "                feat = self.encoder(x)          \n",
    "                feat = torch.flatten(feat, 1)   \n",
    "                return feat\n",
    "\n",
    "       \n",
    "        model = SimSiam()\n",
    "        checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
    "        state_dict = checkpoint['model_state']\n",
    "\n",
    "        \n",
    "        filtered_state_dict = {\n",
    "            k: v for k, v in state_dict.items() if not k.startswith(\"predictor.\")\n",
    "        }\n",
    "\n",
    "        missing, unexpected = model.load_state_dict(filtered_state_dict, strict=False)\n",
    "        print(f\"Missing keys: {missing}\")\n",
    "        print(f\"Unexpected keys: {unexpected}\")\n",
    "\n",
    "        model.eval()\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.encoder = model.encoder\n",
    "\n",
    "    def forward(self, x):\n",
    "        with torch.no_grad():\n",
    "            feat = self.encoder(x)\n",
    "            feat = torch.flatten(feat, 1)\n",
    "        return feat\n",
    "\n",
    "class SimSiamEvalDataset(Dataset):\n",
    "    def __init__(self, folder_path, label_dict, transform):\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        self.class_names = sorted(list(set(label_dict.values())))\n",
    "        self.class_to_idx = {cls: i for i, cls in enumerate(self.class_names)}\n",
    "\n",
    "        for root, _, files in os.walk(folder_path):\n",
    "            for file in files:\n",
    "                if file.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "                    class_folder = os.path.basename(root)\n",
    "                    if class_folder in label_dict:\n",
    "                        self.image_paths.append(os.path.join(root, file))\n",
    "                        self.labels.append(self.class_to_idx[label_dict[class_folder]])\n",
    "\n",
    "        self.transform = transform\n",
    "        print(f\"Found {len(self.image_paths)} labeled images across {len(self.class_names)} classes\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            img = Image.open(self.image_paths[idx]).convert('RGB')\n",
    "            label = self.labels[idx]\n",
    "            if self.transform:\n",
    "                img = self.transform(img)\n",
    "            return img, label\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {self.image_paths[idx]}: {str(e)}\")\n",
    "            return torch.zeros(3, IMG_SIZE, IMG_SIZE), -1\n",
    "\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize(128),\n",
    "    transforms.CenterCrop(IMG_SIZE),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "with open(LABELS_PATH) as f:\n",
    "    label_dict = json.load(f)\n",
    "\n",
    "val_dataset = SimSiamEvalDataset(\n",
    "    folder_path=os.path.join(DATA_ROOT, VAL_FOLDER),\n",
    "    label_dict=label_dict,\n",
    "    transform=val_transform\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "model = SimSiamBackbone(CHECKPOINT_PATH).to(device)\n",
    "model.eval()\n",
    "\n",
    "def extract_features(loader):\n",
    "    features, labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for imgs, lbls in tqdm(loader, desc=\"Extracting features (SimSiam)\"):\n",
    "            imgs = imgs.to(device)\n",
    "            feats = model(imgs).cpu()\n",
    "            features.append(feats)\n",
    "            labels.append(lbls)\n",
    "\n",
    "    features = torch.cat(features)\n",
    "    labels = torch.cat(labels)\n",
    "    valid_idx = labels != -1\n",
    "    return features[valid_idx], labels[valid_idx]\n",
    "\n",
    "X_val, y_val = extract_features(val_loader)\n",
    "print(f\"Total validation images: {len(val_dataset)}\")\n",
    "print(f\"Feature shape: {X_val.shape}, Labels shape: {y_val.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0dc8ef0",
   "metadata": {},
   "source": [
    "## Linear Classifier and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edae807",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_val_scaled = scaler.fit_transform(X_val.numpy())\n",
    "\n",
    "clf = LogisticRegression(\n",
    "    max_iter=5000,\n",
    "    multi_class='multinomial',\n",
    "    solver='saga',  \n",
    "    random_state=42\n",
    ")\n",
    "clf.fit(X_val_scaled, y_val.numpy())\n",
    "\n",
    "\n",
    "val_probs = clf.predict_proba(X_val.numpy())\n",
    "val_preds = np.argmax(val_probs, axis=1)\n",
    "\n",
    "\n",
    "acc = accuracy_score(y_val.numpy(), val_preds)\n",
    "f1 = f1_score(y_val.numpy(), val_preds, average='macro')\n",
    "\n",
    "val_probs_tensor = torch.tensor(val_probs)\n",
    "true_labels_tensor = torch.tensor(y_val.numpy())\n",
    "\n",
    "top1_preds = torch.argmax(val_probs_tensor, dim=1)\n",
    "top1_correct = (top1_preds == true_labels_tensor).sum().item()\n",
    "top1_accuracy = top1_correct / len(y_val)\n",
    "\n",
    "top5_preds = torch.topk(val_probs_tensor, k=5, dim=1).indices\n",
    "top5_correct = torch.any(top5_preds == true_labels_tensor.unsqueeze(1), dim=1).sum().item()\n",
    "top5_accuracy = top5_correct / len(y_val)\n",
    "\n",
    "print(f\"\\nEvaluation Results (SimSiam):\")\n",
    "print(f\"Accuracy (Top-1): {acc*100:.2f}%\")\n",
    "print(f\"Macro F1 Score: {f1*100:.2f}%\")\n",
    "print(f\"Top-1 Accuracy: {top1_accuracy*100:.2f}%\")\n",
    "print(f\"Top-5 Accuracy: {top5_accuracy*100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
