{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c56bf30",
   "metadata": {},
   "source": [
    "## Masked Autoencoders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f335ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "os.listdir(\"/kaggle/input/ssl-dataset/ssl_dataset\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54bc68b",
   "metadata": {},
   "source": [
    "**Parameters and Labels**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f43f0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = \"/kaggle/input/ssl-dataset/ssl_dataset\"\n",
    "TRAIN_FOLDERS = [f\"train.X{i}\" for i in range(1,5)]\n",
    "VAL_FOLDER   = \"val.X\"       \n",
    "LR_PRETRAIN  = 1e-4\n",
    "LR_LINEAR    = 1e-3\n",
    "EPOCHS_PRE   = 20 \n",
    "BATCH_SIZE      = 64       \n",
    "EPOCHS_LINEAR= 50\n",
    "MASK_RATIO   = 0.75       \n",
    "IMG_SIZE     = 224\n",
    "PATCH_SIZE    = 16\n",
    "NUM_WORKERS = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f91910",
   "metadata": {},
   "source": [
    "*Transformations*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa37ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "def _make_dataset(folders):\n",
    "    images = []\n",
    "    for fld in folders:\n",
    "        fld_path = os.path.join(DATA_ROOT, fld)\n",
    "        for root, _, files in os.walk(fld_path):\n",
    "            for fname in files:\n",
    "                if fname.lower().endswith((\".jpg\", \".png\", \".jpeg\")):\n",
    "                    images.append(os.path.join(root, fname))\n",
    "    return images\n",
    "\n",
    "\n",
    "class MAEDataset(Dataset):\n",
    "    def __init__(self, image_paths, transform, patch_size, mask_ratio):\n",
    "        self.image_paths = image_paths\n",
    "        self.transform   = transform\n",
    "        self.patch_size  = patch_size\n",
    "        self.mask_ratio  = mask_ratio\n",
    "        self.num_patches = (IMG_SIZE // patch_size) ** 2\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.image_paths[idx]).convert('RGB')\n",
    "        img = self.transform(img)                  \n",
    "\n",
    "       \n",
    "        patches = self.patchify(img)              \n",
    "\n",
    "        \n",
    "        num_mask = int(self.mask_ratio * self.num_patches)\n",
    "        perm     = torch.randperm(self.num_patches)\n",
    "        mask_idx = perm[:num_mask]\n",
    "        mask     = torch.zeros(self.num_patches, dtype=torch.bool)\n",
    "        mask[mask_idx] = True\n",
    "\n",
    "        \n",
    "        patches_masked = patches.clone()\n",
    "        patches_masked[mask] = 0\n",
    "        img_masked     = self.unpatchify(patches_masked) \n",
    "\n",
    "        return {\n",
    "            'img_masked': img_masked,\n",
    "            'mask':       mask,\n",
    "            'img_orig':   img\n",
    "        }\n",
    "\n",
    "    def patchify(self, img):\n",
    "      \n",
    "        p = self.patch_size\n",
    "        c, h, w = img.shape\n",
    "        x = img.reshape(c, h//p, p, w//p, p)\n",
    "        x = x.permute(1, 3, 0, 2, 4)   \n",
    "        return x.reshape(-1, c * p * p)\n",
    "\n",
    "    def unpatchify(self, patches):\n",
    "       \n",
    "        p = self.patch_size\n",
    "        c = 3\n",
    "        nh = nw = IMG_SIZE // p\n",
    "        x  = patches.reshape(nh, nw, c, p, p)\n",
    "        x  = x.permute(2, 0, 3, 1, 4)  \n",
    "        return x.reshape(c, IMG_SIZE, IMG_SIZE)\n",
    "\n",
    "\n",
    "image_paths   = _make_dataset(TRAIN_FOLDERS)\n",
    "import random\n",
    "\n",
    "\n",
    "image_paths = _make_dataset(TRAIN_FOLDERS)\n",
    "\n",
    "random.seed(42)  \n",
    "if len(image_paths) > 80000:\n",
    "    image_paths = random.sample(image_paths, 90000)\n",
    "train_dataset = MAEDataset(\n",
    "    image_paths=image_paths,\n",
    "    transform=train_transform,\n",
    "    patch_size=PATCH_SIZE,\n",
    "    mask_ratio=MASK_RATIO\n",
    ")\n",
    "train_loader  = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory= False\n",
    ")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    with torch.no_grad():\n",
    "        batch = next(iter(train_loader))\n",
    "    print('img_masked:', batch['img_masked'].shape)  \n",
    "    print('mask:',       batch['mask'].shape)        \n",
    "    print('img_orig:',   batch['img_orig'].shape)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6eeb42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "import torch.nn as nn\n",
    "\n",
    "# Device setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "# (tiny)\n",
    "encoder = timm.create_model('vit_tiny_patch16_224', pretrained=True)\n",
    "\n",
    "\n",
    "# 2) Remove its classification head\n",
    "encoder.reset_classifier(0)  \n",
    "\n",
    "# 3) Expose key pieces\n",
    "patch_embed = encoder.patch_embed       \n",
    "pos_embed   = encoder.pos_embed         \n",
    "encoder_blocks = encoder.blocks          \n",
    "encoder_norm   = encoder.norm            \n",
    "embed_dim      = encoder.embed_dim       \n",
    "num_patches    = patch_embed.num_patches \n",
    "\n",
    "# 4) Learnable mask token\n",
    "mask_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "\n",
    "# 5) Simple decoder: 8 Transformer layers, with batch_first=True\n",
    "decoder_layer = nn.TransformerEncoderLayer(\n",
    "    d_model=embed_dim,\n",
    "    nhead=4,\n",
    "    dim_feedforward=embed_dim*2,\n",
    "    batch_first=True            # <-- ensures inputs are [B, N, D]\n",
    ")\n",
    "decoder = nn.TransformerEncoder(decoder_layer, num_layers=4)\n",
    "\n",
    "# 6) Reconstruction head: map each D-dimensional token â†’ patch pixels\n",
    "patch_size = patch_embed.patch_size[0] \n",
    "reconstruction_head = nn.Linear(\n",
    "    embed_dim,\n",
    "    patch_size * patch_size * 3\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401093ab",
   "metadata": {},
   "source": [
    "**Custom MAE Design**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0034549f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomMAE(nn.Module):\n",
    "    def __init__(self, encoder, patch_embed, pos_embed,\n",
    "                 encoder_blocks, encoder_norm,\n",
    "                 mask_token, decoder, reconstruction_head,\n",
    "                 patch_size=16):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.encoder          = encoder\n",
    "        self.patch_embed      = patch_embed\n",
    "        self.pos_embed        = pos_embed\n",
    "        self.encoder_blocks   = encoder_blocks\n",
    "        self.encoder_norm     = encoder_norm\n",
    "        self.mask_token       = mask_token\n",
    "        self.decoder          = decoder\n",
    "        self.reconstruction_head = reconstruction_head\n",
    "\n",
    "        self.embed_dim = pos_embed.size(-1)\n",
    "\n",
    "    def patchify(self, imgs):\n",
    "        p = self.patch_size\n",
    "        assert imgs.shape[2] % p == 0 and imgs.shape[3] % p == 0\n",
    "        h = imgs.shape[2] // p\n",
    "        w = imgs.shape[3] // p\n",
    "        x = imgs.reshape(shape=(imgs.shape[0], 3, h, p, w, p))\n",
    "        x = torch.einsum('nchpwq->nhwpqc', x)\n",
    "        x = x.reshape(shape=(imgs.shape[0], h * w, p**2 * 3))\n",
    "        return x\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        B = x.size(0)\n",
    "        x = self.patch_embed(x)                   \n",
    "        x = x + self.pos_embed[:, 1:, :]          \n",
    "\n",
    "        flat_x = x.reshape(B * x.size(1), -1)\n",
    "        flat_mask = mask.unsqueeze(-1).expand(-1, -1, x.size(-1))\n",
    "        flat_mask = flat_mask.reshape(B * x.size(1), -1)[:,0].bool()\n",
    "        unmasked = flat_x[~flat_mask].reshape(B, -1, x.size(-1))\n",
    "\n",
    "        enc = unmasked\n",
    "        for blk in self.encoder_blocks:\n",
    "            enc = blk(enc)\n",
    "        enc = self.encoder_norm(enc)\n",
    "\n",
    "        dec_seq = torch.zeros(B, x.size(1), self.embed_dim, device=x.device)\n",
    "        for i in range(B):\n",
    "            unmasked_idx = ~mask[i]\n",
    "            masked_idx   = mask[i]\n",
    "            dec_seq[i, unmasked_idx] = enc[i]\n",
    "    \n",
    "            num_masked = masked_idx.sum()\n",
    "      \n",
    "            mask_tokens = self.mask_token.expand(1, num_masked, self.embed_dim).squeeze(0)\n",
    "    \n",
    "            dec_seq[i, masked_idx] = mask_tokens\n",
    "\n",
    "        dec_seq = dec_seq + self.pos_embed[:, 1:, :]\n",
    "\n",
    "        dec_input = dec_seq.permute(1, 0, 2)\n",
    "        dec_out   = self.decoder(dec_input)\n",
    "        dec_out   = dec_out.permute(1, 0, 2)\n",
    "\n",
    "\n",
    "      \n",
    "        masked_out = []\n",
    "        for i in range(B):\n",
    "            masked_out.append(dec_out[i][mask[i]])\n",
    "        masked_out = torch.stack([F.pad(mo, (0, 0, 0, mask.sum(1).max() - mo.size(0))) for mo in masked_out])\n",
    "\n",
    "        preds = self.reconstruction_head(masked_out)\n",
    "        return preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c33b4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CustomMAE(\n",
    "    encoder, patch_embed, pos_embed,\n",
    "    encoder_blocks, encoder_norm,\n",
    "    mask_token, decoder, reconstruction_head\n",
    ").to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4443510",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "criterion = nn.MSELoss(reduction='none')\n",
    "# AdamW optimizer\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LR_PRETRAIN, weight_decay=0.05)\n",
    "# Cosine LR schedule with linear warmup\n",
    "total_steps = len(train_loader) * EPOCHS_PRE\n",
    "warmup_steps = int(0.1 * total_steps)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer,\n",
    "    T_max=total_steps - warmup_steps,\n",
    "    eta_min=1e-6\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b307f9d",
   "metadata": {},
   "source": [
    "**Pre-Train Loop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b4ef3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "\n",
    "os.makedirs(\"/kaggle/working/\", exist_ok=True)\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "scaler = GradScaler()\n",
    "\n",
    "for epoch in range(1, EPOCHS_PRE + 1):\n",
    "    epoch_loss = 0.0\n",
    "    loop = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch}/{EPOCHS_PRE}\")\n",
    "\n",
    "    for batch_idx, batch in loop:\n",
    "        imgs_masked = batch['img_masked'].to(device)\n",
    "        masks       = batch['mask'].to(device)\n",
    "        imgs_orig   = batch['img_orig'].to(device)\n",
    "\n",
    "        with autocast():  \n",
    "            preds = model(imgs_masked, mask=masks)\n",
    "            with torch.no_grad():\n",
    "                patchified = model.patchify(imgs_orig)\n",
    "\n",
    "            B, L = masks.shape\n",
    "            N, D = patchified.shape[1], patchified.shape[2]\n",
    "            target_patches = []\n",
    "            max_masked = masks.sum(dim=1).max()\n",
    "\n",
    "            for b in range(B):\n",
    "                masked_indices = masks[b].bool()\n",
    "                selected = patchified[b][masked_indices]\n",
    "                pad_size = max_masked - selected.shape[0]\n",
    "                if pad_size > 0:\n",
    "                    selected = F.pad(selected, (0, 0, 0, pad_size))  \n",
    "                target_patches.append(selected)\n",
    "\n",
    "            target_patches = torch.stack(target_patches).to(device)\n",
    "            loss = ((preds - target_patches) ** 2).mean()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        current_step = (epoch - 1) * len(train_loader) + batch_idx\n",
    "        if current_step <= warmup_steps:\n",
    "            lr = 1e-6 + (LR_PRETRAIN - 1e-6) * (current_step / warmup_steps)\n",
    "            for pg in optimizer.param_groups:\n",
    "                pg['lr'] = lr\n",
    "        else:\n",
    "            scheduler.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        loop.set_postfix(loss=loss.item(), lr=optimizer.param_groups[0]['lr'])\n",
    "\n",
    "    avg_loss = epoch_loss / len(train_loader)\n",
    "    print(f\"\\nâœ… Epoch {epoch}/{EPOCHS_PRE} â€” Avg Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    if True:\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'loss': avg_loss,\n",
    "        }\n",
    "        torch.save(checkpoint, f\"/kaggle/working/mae_checkpoint_epoch{epoch}.pth\")\n",
    "        print(f\"ðŸ’¾ Checkpoint saved at epoch {epoch}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
