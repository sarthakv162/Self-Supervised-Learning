{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0d6ff22",
   "metadata": {},
   "source": [
    "# MAE Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731368ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import seaborn as sns\n",
    "import timm\n",
    "\n",
    "# Device setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Paths\n",
    "DATA_ROOT = \"/kaggle/input/ssl-dataset/ssl_dataset\"\n",
    "CHECKPOINT_PATH = \"/kaggle/input/maemodel/pytorch/default/1/mae_checkpoint_epoch50.pth\"\n",
    "VAL_FOLDER = \"val.X\"\n",
    "LABELS_PATH = os.path.join(DATA_ROOT, \"Labels.json\")\n",
    "\n",
    "# Hyperparameters\n",
    "BATCH_SIZE = 64\n",
    "IMG_SIZE = 224\n",
    "\n",
    "# --- MAE Model Loading ---\n",
    "# (Assumes you have defined CustomMAE elsewhere, can import it or run code in continue)\n",
    "encoder = timm.create_model('vit_tiny_patch16_224', pretrained=False)\n",
    "encoder.reset_classifier(0)\n",
    "\n",
    "patch_embed     = encoder.patch_embed\n",
    "pos_embed       = encoder.pos_embed\n",
    "encoder_blocks  = encoder.blocks\n",
    "encoder_norm    = encoder.norm\n",
    "embed_dim       = encoder.embed_dim\n",
    "\n",
    "mask_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "decoder_layer = nn.TransformerEncoderLayer(\n",
    "    d_model=embed_dim,\n",
    "    nhead=4,\n",
    "    dim_feedforward=embed_dim*2,\n",
    "    batch_first=True\n",
    ")\n",
    "decoder = nn.TransformerEncoder(decoder_layer, num_layers=4)\n",
    "reconstruction_head = nn.Linear(embed_dim, 16*16*3)\n",
    "\n",
    "model = CustomMAE(\n",
    "    encoder=encoder,\n",
    "    patch_embed=patch_embed,\n",
    "    pos_embed=pos_embed,\n",
    "    encoder_blocks=encoder_blocks,\n",
    "    encoder_norm=encoder_norm,\n",
    "    mask_token=mask_token,\n",
    "    decoder=decoder,\n",
    "    reconstruction_head=reconstruction_head\n",
    ").to(device)\n",
    "\n",
    "checkpoint = torch.load(CHECKPOINT_PATH, map_location=device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "def get_features(self, x):\n",
    "    x = self.patch_embed(x)\n",
    "    x = x + self.pos_embed[:, 1:, :]\n",
    "    for blk in self.encoder_blocks:\n",
    "        x = blk(x)\n",
    "    x = self.encoder_norm(x)\n",
    "    return x.mean(dim=1)\n",
    "\n",
    "model.get_features = get_features.__get__(model)\n",
    "\n",
    "# --- Dataset Class ---\n",
    "class MAEEvalDataset(Dataset):\n",
    "    def __init__(self, folder_path, label_dict, transform):\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        self.class_names = sorted(list(set(label_dict.values())))\n",
    "        self.class_to_idx = {cls: i for i, cls in enumerate(self.class_names)}\n",
    "        \n",
    "        for root, _, files in os.walk(folder_path):\n",
    "            for file in files:\n",
    "                if file.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "                    cls_folder = os.path.basename(root)\n",
    "                    if cls_folder in label_dict:\n",
    "                        self.image_paths.append(os.path.join(root, file))\n",
    "                        self.labels.append(self.class_to_idx[label_dict[cls_folder]])\n",
    "        \n",
    "        self.transform = transform\n",
    "        print(f\"Found {len(self.image_paths)} labeled images across {len(self.class_names)} classes\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            img = Image.open(self.image_paths[idx]).convert('RGB')\n",
    "            label = self.labels[idx]\n",
    "            if self.transform:\n",
    "                img = self.transform(img)\n",
    "            return img, label\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {self.image_paths[idx]}: {e}\")\n",
    "            return torch.zeros(3, IMG_SIZE, IMG_SIZE), -1\n",
    "\n",
    "# --- Transforms ---\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(IMG_SIZE),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# --- Load Labels & DataLoader ---\n",
    "with open(LABELS_PATH) as f:\n",
    "    label_dict = json.load(f)\n",
    "\n",
    "val_dataset = MAEEvalDataset(\n",
    "    folder_path=os.path.join(DATA_ROOT, VAL_FOLDER),\n",
    "    label_dict=label_dict,\n",
    "    transform=val_transform\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")\n",
    "print(f\"Total validation images: {len(val_dataset)}\")\n",
    "\n",
    "# --- Feature Extraction ---\n",
    "def extract_features(loader):\n",
    "    feats_list, lbls_list = [], []\n",
    "    with torch.no_grad():\n",
    "        for imgs, lbls in tqdm(loader, desc=\"Extracting features\"):\n",
    "            imgs = imgs.to(device)\n",
    "            feats = model.get_features(imgs).cpu()\n",
    "            feats_list.append(feats)\n",
    "            lbls_list.append(lbls)\n",
    "    feats = torch.cat(feats_list)\n",
    "    lbls = torch.cat(lbls_list)\n",
    "    valid = lbls != -1\n",
    "    return feats[valid], lbls[valid]\n",
    "\n",
    "X_val, y_val = extract_features(val_loader)\n",
    "print(f\"Feature shape: {X_val.shape}, Labels shape: {y_val.shape}\")\n",
    "\n",
    "# --- Linear Probing ---\n",
    "clf = LogisticRegression(\n",
    "    max_iter=1000,\n",
    "    multi_class='multinomial',\n",
    "    solver='lbfgs',\n",
    "    random_state=42\n",
    ")\n",
    "clf.fit(X_val.numpy(), y_val.numpy())\n",
    "\n",
    "# --- Evaluation ---\n",
    "val_probs = clf.predict_proba(X_val.numpy())\n",
    "val_preds = np.argmax(val_probs, axis=1)\n",
    "\n",
    "acc = accuracy_score(y_val.numpy(), val_preds)\n",
    "f1  = f1_score(y_val.numpy(), val_preds, average='macro')\n",
    "\n",
    "# Top-1 & Top-5\n",
    "probs_tensor = torch.tensor(val_probs)\n",
    "true_tensor = torch.tensor(y_val.numpy())\n",
    "\n",
    "top1 = (torch.argmax(probs_tensor,1) == true_tensor).float().mean().item()\n",
    "top5 = (torch.any(torch.topk(probs_tensor,5,1).indices == true_tensor.unsqueeze(1),1)\n",
    "        .float().mean().item())\n",
    "\n",
    "print(\"\\nEvaluation Results:\")\n",
    "print(f\"F1 Score (Macro):  {f1*100:.2f}%\")\n",
    "print(f\"Top-1 Accuracy:     {top1*100:.2f}%\")\n",
    "print(f\"Top-5 Accuracy:     {top5*100:.2f}%\")\n",
    "\n",
    "# --- Confusion Matrix ---\n",
    "cm = confusion_matrix(y_val.numpy(), val_preds)\n",
    "class_names = val_dataset.class_names\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=class_names,\n",
    "            yticklabels=class_names)\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- Training History Visualization ---\n",
    "def plot_training_history():\n",
    "    # Try to get training history from checkpoint\n",
    "    if 'train_loss_history' in checkpoint and 'val_loss_history' in checkpoint:\n",
    "        train_loss = checkpoint['train_loss_history']\n",
    "        val_loss = checkpoint['val_loss_history']\n",
    "        train_acc = checkpoint.get('train_acc_history', [])\n",
    "        val_acc = checkpoint.get('val_acc_history', [])\n",
    "        \n",
    "        epochs = len(train_loss)\n",
    "        \n",
    "        plt.figure(figsize=(12, 5))\n",
    "        \n",
    "        # Loss plot\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(range(1, epochs+1), train_loss, label='Training Loss')\n",
    "        plt.plot(range(1, epochs+1), val_loss, label='Validation Loss')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Training and Validation Loss')\n",
    "        plt.legend()\n",
    "        \n",
    "       \n",
    "        if train_acc and val_acc:\n",
    "            plt.subplot(1, 2, 2)\n",
    "            plt.plot(range(1, epochs+1), train_acc, label='Training Accuracy')\n",
    "            plt.plot(range(1, epochs+1), val_acc, label='Validation Accuracy')\n",
    "            plt.xlabel('Epochs')\n",
    "            plt.ylabel('Accuracy')\n",
    "            plt.title('Training and Validation Accuracy')\n",
    "            plt.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('/kaggle/working/training_history.png')\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"No training history found in checkpoint\")\n",
    "\n",
    "plot_training_history()\n",
    "\n",
    "# --- Save Results\n",
    "with open('/kaggle/working/results.txt', 'w') as f:\n",
    "    f.write(f\"Accuracy: {acc*100:.2f}%\\n\")\n",
    "    f.write(f\"Macro F1 Score: {f1*100:.2f}%\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795eafcd",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
